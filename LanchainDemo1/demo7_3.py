from youtube_transcript_api import YouTubeTranscriptApi
from pytube import YouTube
import datetime
import os

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pydantic.v1 import BaseModel, Field
from typing import Optional, List
import yt_dlp
import datetime
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma

# # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["LANGCHAIN_PROJECT"] = "LangchainDemo5"


import yt_dlp
# ğŸ”¹åˆ›å»º OpenAI ä»£ç†ï¼ˆä¸ä¼  `system_message`ï¼‰
model = ChatOpenAI(
    openai_api_key="sk-84f2472c5c8042e0a23927b4176bdfe9",
    base_url="https://api.deepseek.com/v1",
    model_name="deepseek-chat"
)


# DeepSeek å…¼å®¹çš„ Embedding æ¨¡å‹
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-base-zh")  # é€‚ç”¨äºä¸­æ–‡

persist_dir = 'chroma_data_dir1'  # å­˜æ”¾å‘é‡æ•°æ®åº“çš„ç›®å½•

# ä¸€äº›YouTubeçš„è§†é¢‘è¿æ¥
urls = [
    "https://www.youtube.com/watch?v=dA1cHGACXCo",
    "https://www.youtube.com/watch?v=ZcEMLz27sL4",
    "https://www.youtube.com/watch?v=hvAPnpSfSGo",
    "https://www.youtube.com/watch?v=EhlPDL4QrWY",
    "https://www.youtube.com/watch?v=mmBo8nlu2j0",
    "https://www.youtube.com/watch?v=rQdibOsL1ps",
    "https://www.youtube.com/watch?v=28lC4fqukoc",
    "https://www.youtube.com/watch?v=es-9MgxB-uc",
    "https://www.youtube.com/watch?v=wLRHwKuKvOE",
    "https://www.youtube.com/watch?v=ObIltMaRJvY",
]

# ç”¨äºå­˜å‚¨æ–‡æ¡£
docs = []


# ä½¿ç”¨yt-dlpè·å–è§†é¢‘æ•°æ®
def fetch_video_info(url):
    ydl_opts = {
        'format': 'bestaudio/best',  # é€‰æ‹©è§†é¢‘è´¨é‡
        'extractaudio': True,  # è·å–éŸ³é¢‘
        'writeinfojson': True,  # æå–è§†é¢‘ä¿¡æ¯
        'quiet': True,  # ç¦æ­¢æ˜¾ç¤ºä¸‹è½½è¿›åº¦
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(url, download=False)  # ä¸ä¸‹è½½è§†é¢‘ï¼Œåªæå–ä¿¡æ¯
        subtitles = info_dict.get("subtitles", {})

        # è·å–å­—å¹•ï¼ˆå¦‚æœæœ‰ï¼‰
        subtitle_text = ""
        if 'en' in subtitles:
            subtitle_text = "\n".join(subtitles['en'])

        # å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œå°±ç”¨è§†é¢‘æ ‡é¢˜
        if not subtitle_text:
            subtitle_text = info_dict.get("title", "")

        metadata = {
            "title": info_dict.get("title"),
            "publish_date": info_dict.get("upload_date"),
            "video_url": url,
            "subtitle_text": subtitle_text
        }

        # åˆ›å»ºæ–‡æ¡£
        doc = Document(
            page_content=subtitle_text,
            metadata=metadata
        )
        return doc


# å¤„ç†æ‰€æœ‰è§†é¢‘é“¾æ¥
for url in urls:
    docs.append(fetch_video_info(url))

# print(len(docs))
# print(docs[0].metadata)
# print(docs[0].page_content[:500])  # ç¬¬ä¸€ä¸ªè§†é¢‘çš„å­—å¹•å†…å®¹

# ç»™docæ·»åŠ é¢å¤–çš„å…ƒæ•°æ®ï¼šè§†é¢‘å‘å¸ƒçš„å¹´ä»½
for doc in docs:
    doc.metadata['publish_year'] = int(datetime.datetime.strptime(doc.metadata['publish_date'], '%Y%m%d').strftime('%Y'))

# æ ¹æ®å¤šä¸ªdocæ„å»ºå‘é‡æ•°æ®åº“
text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=30)
split_doc = text_splitter.split_documents(docs)

# å‘é‡æ•°æ®åº“çš„æŒä¹…åŒ–
vectorstore = Chroma.from_documents(split_doc, embedding_model, persist_directory='chroma_data_dir')