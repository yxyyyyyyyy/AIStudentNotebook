import datetime
import os

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pydantic.v1 import BaseModel, Field
from typing import Optional, List
import yt_dlp
import datetime
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma

# # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["LANGCHAIN_PROJECT"] = "LangchainDemo5"

# ğŸ”¹åˆ›å»º OpenAI ä»£ç†ï¼ˆä¸ä¼  `system_message`ï¼‰
model = ChatOpenAI(
    openai_api_key="sk-84f2472c5c8042e0a23927b4176bdfe9",
    base_url="https://api.deepseek.com/v1",
    model_name="deepseek-chat"
)


# DeepSeek å…¼å®¹çš„ Embedding æ¨¡å‹
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-base-zh")  # é€‚ç”¨äºä¸­æ–‡

persist_dir = 'chroma_data_dir'  # å­˜æ”¾å‘é‡æ•°æ®åº“çš„ç›®å½•

# åŠ è½½ç£ç›˜ä¸­çš„å‘é‡æ•°æ®åº“
vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embedding_model)
result = vectorstore.similarity_search_with_score('How can we build an agent of RAG?')
# print(result[0])
# print(result[0][0].metadata['publish_year'])

system = """You are an expert at converting user questions into database queries. \
You have access to a database of tutorial videos about a software library for building LLM-powered applications. \
Given a question, return a list of database queries optimized to retrieve the most relevant results.

If there are acronyms or words you are not familiar with, do not try to rephrase them."""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)

# pydantic: ä¸“é—¨åšæ•°æ®ç®¡ç†çš„åº“
# å®šä¹‰æ•°æ®æ¨¡å‹å¾—åˆ°æ£€ç´¢æŒ‡ä»¤
class Search(BaseModel):
    """
    å®šä¹‰äº†ä¸€ä¸ªæ•°æ®æ¨¡å‹
    """
    # å†…å®¹çš„ç›¸ä¼¼æ€§å’Œå‘å¸ƒå¹´ä»½
    query: str = Field(None, description='Similarity search query applied to video transcripts.')
    publish_year: Optional[int] = Field(None, description='Year video was published')

chain = {'question': RunnablePassthrough()} | prompt | model.with_structured_output(Search)

# resp1 = chain.invoke('how do I build a RAG agent?')
# print(resp1)
# resp2 = chain.invoke('videos on RAG published in 2023')
# print(resp2)

# åŸºäºç‰¹å®šçš„å€¼çš„æœç´¢ - æ ¹æ®æ£€ç´¢æ¡ä»¶å»æ‰§è¡Œ
def retrieval(search: Search) -> List[Document]:
    _filter = None
    if search.publish_year:
        # æ ¹æ®publish_yearï¼Œå­˜åœ¨å¾—åˆ°ä¸€ä¸ªæ£€ç´¢æ¡ä»¶
        # "$eq"æ˜¯Chromaå‘é‡æ•°æ®åº“çš„å›ºå®šè¯­æ³•
        _filter = {'publish_year': {"$eq": search.publish_year}}

    return vectorstore.similarity_search(search.query, filter=_filter)

new_chain = chain | retrieval

result = new_chain.invoke('videos on RAG published in 2023')
# result = new_chain.invoke('RAG tutorial')
print([(doc.metadata['title'], doc.metadata['publish_year']) for doc in result])

